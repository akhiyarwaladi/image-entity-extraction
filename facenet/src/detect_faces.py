#!/usr/bin/python
# -*- coding: utf-8 -*-
# pylint: disable=C0103
# pylint: disable=E1101

import sys
import time
import numpy as np
import tensorflow as tf
import cv2
import argparse
import tiny_face_model
import glob
import os
import pickle
import pylab as pl
from scipy.special import expit

import label_map_util
import visualization_utils_color as vis_util

MAX_INPUT_DIM = 5000.0



# Path to frozen detection graph. This is the actual model that is used for the object detection.
PATH_TO_CKPT = './Models/frozen_inference_graph_face.pb'

# List of the strings that is used to add correct label for each box.
PATH_TO_LABELS = './Models/face_label_map.pbtxt'

NUM_CLASSES = 2

label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

def load_image_into_numpy_array(image):
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)

# cap = cv2.VideoCapture("./media/test.mp4")
# out = None
def detect_face(image):
  # parser = argparse.ArgumentParser()
  # parser.add_argument('--path', help = 'Path of the video you want to test on.', default = 0)
  # args = parser.parse_args()

  detection_graph = tf.Graph()
  with detection_graph.as_default():
      od_graph_def = tf.GraphDef()
      with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
          serialized_graph = fid.read()
          od_graph_def.ParseFromString(serialized_graph)
          tf.import_graph_def(od_graph_def, name='')

  with detection_graph.as_default():
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    with tf.Session(graph=detection_graph, config=config) as sess:

      image_np = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

      # the array based representation of the image will be used later in order to prepare the
      # result image with boxes and labels on it.
      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
      image_np_expanded = np.expand_dims(image_np, axis=0)
      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
      # Each box represents a part of the image where a particular object was detected.
      boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
      # Each score represent how level of confidence for each of the objects.
      # Score is shown on the result image, together with the class label.
      scores = detection_graph.get_tensor_by_name('detection_scores:0')
      classes = detection_graph.get_tensor_by_name('detection_classes:0')
      num_detections = detection_graph.get_tensor_by_name('num_detections:0')
      # Actual detection.
      start_time = time.time()
      (boxes, scores, classes, num_detections) = sess.run(
          [boxes, scores, classes, num_detections],
          feed_dict={image_tensor: image_np_expanded})
      elapsed_time = time.time() - start_time
      print('inference time cost: {}'.format(elapsed_time))
      print(boxes.shape, boxes)
      print(scores.shape, scores)
      # print(classes.shape, classes)
      # print(num_detections)

      # # Visualization of the results of a detection.
      # bboxes = vis_util.visualize_boxes_and_labels_on_image_array(

      #     image,
      #     np.squeeze(boxes),
      #     np.squeeze(classes).astype(np.int32),
      #     np.squeeze(scores),
      #     category_index,
      #     use_normalized_coordinates=True,
      #     line_thickness=4)

      # print(np.array(bboxes))
      min_score_thresh=.6
      boxes  = np.squeeze(boxes)
      scores = np.squeeze(scores)
      bboxes = []
      boxes_shape = boxes.shape
      for i in range(boxes_shape[0]):
        if scores is None or scores[i] > min_score_thresh:
          ymin, xmin, ymax, xmax = boxes[i, 0], boxes[i, 1], boxes[i, 2], boxes[i, 3]
          im_height, im_width, _ = image_np.shape

          left, right, top, bottom = (xmin * im_width), (xmax * im_width), (ymin * im_height), (ymax * im_height)

          bboxes.append([left, top, right, bottom])
          print(left, right, top, bottom)
          cv2.imshow('Face Recognition',image_np[int(top):int(bottom), int(left):int(right)])
          cv2.waitKey(0)

      return np.array(bboxes)

def detect_tiny_face(image, prob_thresh=0.5, nms_thresh=0.1, lw=3):
  """Detect faces in images.
  Args:
    prob_thresh:
        The threshold of detection confidence.
    nms_thresh:
        The overlap threshold of non maximum suppression
    weight_file_path: 
        A pretrained weight file in the pickle format 
        generated by matconvnet_hr101_to_tf.py.
    data_dir: 
        A directory which contains images.
    output_dir: 
        A directory into which images with detected faces are output.
    lw: 
        Line width of bounding boxes. If zero specified,
        this is determined based on confidence of each detection.
    display:
        Display tiny face images on window.
  Returns:
    None.
  """
  # weight_file_path = "/Users/akhiyarwaladi/Downloads/hr_res101.pkl"
  # placeholder of input images. Currently batch size of one is supported.
  weight_file_path = "/Users/akhiyarwaladi/Downloads/hr_res101.pkl"
  x = tf.placeholder(tf.float32, [1, None, None, 3]) # n, h, w, c

  # Create the tiny face model which weights are loaded from a pretrained model.
  model = tiny_face_model.Model(weight_file_path)
  score_final = model.tiny_face(x)

  # # Find image files in data_dir.
  # filenames = []
  # for ext in ('*.png', '*.gif', '*.jpg', '*.jpeg'):
  #   filenames.extend(glob.glob(os.path.join(data_dir, ext)))

  # Load an average image and clusters(reference boxes of templates).
  with open(weight_file_path, "rb") as f:
    _, mat_params_dict = pickle.load(f)

  average_image = model.get_data_by_key("average_image")
  clusters = model.get_data_by_key("clusters")
  clusters_h = clusters[:, 3] - clusters[:, 1] + 1
  clusters_w = clusters[:, 2] - clusters[:, 0] + 1
  normal_idx = np.where(clusters[:, 4] == 1)

  # main
  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    # # for filename in filenames:
    # filename = im_path
    # fname = filename.split(os.sep)[-1]
    fname = 'fname'
    # raw_img = cv2.imread(filename)
    raw_img = image.copy()
    raw_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB)
    raw_img_f = raw_img.astype(np.float32)

    def _calc_scales():
      raw_h, raw_w = raw_img.shape[0], raw_img.shape[1]
      min_scale = min(np.floor(np.log2(np.max(clusters_w[normal_idx] / raw_w))),
                      np.floor(np.log2(np.max(clusters_h[normal_idx] / raw_h))))
      max_scale = min(1.0, -np.log2(max(raw_h, raw_w) / MAX_INPUT_DIM))
      scales_down = pl.frange(min_scale, 0, 1.)
      scales_up = pl.frange(0.5, max_scale, 0.5)
      scales_pow = np.hstack((scales_down, scales_up))
      scales = np.power(2.0, scales_pow)
      return scales

    scales = _calc_scales()
    start = time.time()

    # initialize output
    bboxes = np.empty(shape=(0, 5))

    # process input at different scales
    for s in scales:
      print("Processing {} at scale {:.4f}".format(fname, s))
      img = cv2.resize(raw_img_f, (0, 0), fx=s, fy=s, interpolation=cv2.INTER_LINEAR)
      img = img - average_image
      img = img[np.newaxis, :]

      # we don't run every template on every scale ids of templates to ignore
      tids = list(range(4, 12)) + ([] if s <= 1.0 else list(range(18, 25)))
      ignoredTids = list(set(range(0, clusters.shape[0])) - set(tids))

      # run through the net
      score_final_tf = sess.run(score_final, feed_dict={x: img})

      # collect scores
      score_cls_tf, score_reg_tf = score_final_tf[:, :, :, :25], score_final_tf[:, :, :, 25:125]
      prob_cls_tf = expit(score_cls_tf)
      prob_cls_tf[0, :, :, ignoredTids] = 0.0

      def _calc_bounding_boxes():
        # threshold for detection
        _, fy, fx, fc = np.where(prob_cls_tf > prob_thresh)

        # interpret heatmap into bounding boxes
        cy = fy * 8 - 1
        cx = fx * 8 - 1
        ch = clusters[fc, 3] - clusters[fc, 1] + 1
        cw = clusters[fc, 2] - clusters[fc, 0] + 1

        # extract bounding box refinement
        Nt = clusters.shape[0]
        tx = score_reg_tf[0, :, :, 0:Nt]
        ty = score_reg_tf[0, :, :, Nt:2*Nt]
        tw = score_reg_tf[0, :, :, 2*Nt:3*Nt]
        th = score_reg_tf[0, :, :, 3*Nt:4*Nt]

        # refine bounding boxes
        dcx = cw * tx[fy, fx, fc]
        dcy = ch * ty[fy, fx, fc]
        rcx = cx + dcx
        rcy = cy + dcy
        rcw = cw * np.exp(tw[fy, fx, fc])
        rch = ch * np.exp(th[fy, fx, fc])

        scores = score_cls_tf[0, fy, fx, fc]
        tmp_bboxes = np.vstack((rcx - rcw / 2, rcy - rch / 2, rcx + rcw / 2, rcy + rch / 2))
        tmp_bboxes = np.vstack((tmp_bboxes / s, scores))
        tmp_bboxes = tmp_bboxes.transpose()
        return tmp_bboxes

      tmp_bboxes = _calc_bounding_boxes()
      bboxes = np.vstack((bboxes, tmp_bboxes)) # <class 'tuple'>: (5265, 5)



    print("time {:.2f} secs for {}".format(time.time() - start, fname))
    print(bboxes)
    # non maximum suppression
    # refind_idx = util.nms(bboxes, nms_thresh)
    refind_idx = tf.image.non_max_suppression(tf.convert_to_tensor(bboxes[:, :4], dtype=tf.float32),
                                                 tf.convert_to_tensor(bboxes[:, 4], dtype=tf.float32),
                                                 max_output_size=bboxes.shape[0], iou_threshold=nms_thresh)
    refind_idx = sess.run(refind_idx)
    refined_bboxes = bboxes[refind_idx]
    return refined_bboxes
 

if __name__ == '__main__':
  detect_face()